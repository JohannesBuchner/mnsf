import numpy
import os,sys
import astropy.io.fits as pyfits
import math
import scipy.optimize
import warnings

#This is to speed up
log                           = numpy.log

#Activate this to compare with ROOT results
test                          = False



class LogLikelihood(object):
  '''
  Implements a Poisson likelihood (i.e., the Cash statistic). Mind that this is not
  the Castor statistic (Cstat). The difference between the two is a constant given
  a dataset. I kept Cash instead of Castor to make easier the comparison with ROOT
  during tests, since ROOT implements the Cash statistic.
  '''
  def __init__(self,x,y,model,**kwargs):
    self.x                    = x
    self.y                    = y
    self.model                = model
    self.parameters           = model.getParams()
    
    #Initialize the exposure to 1.0 (i.e., non-influential)
    #It will be replaced by the real exposure if the exposure keyword
    #have been used
    self.exposure             = numpy.zeros(len(x))+1.0
        
    for key in kwargs.keys():
      if  (key.lower()=="exposure"):            
        self.exposure = numpy.array(kwargs[key])
    pass  
        
  pass
  
  def _evalLogM(self,M):
    #Evaluate the logarithm with protection for negative or small
    #numbers, using a smooth linear extrapolation (better than just a sharp
    #cutoff)
    tiny                      = numpy.float64(numpy.finfo(M[0]).tiny)
    
    nontinyMask               = (M > 2.0*tiny)
    tinyMask                  = numpy.logical_not(nontinyMask)
    
    if(len(tinyMask.nonzero()[0])>0):      
      logM                     = numpy.zeros(len(M))
      logM[tinyMask]           = numpy.abs(M[tinyMask])/tiny + log(tiny) -1
      logM[nontinyMask]        = log(M[nontinyMask])
    else:
      logM                     = log(M)
    return logM
  pass
  
  def __call__(self, parameters):
    '''
      Evaluate the Cash statistic for the given set of parameters
    '''
    
    #Compute the values for the model given this set of parameters
    self.model.setParams(parameters)
    M                         = self.model(self.x)*self.exposure
    Mfixed,tiny               = self._fixPrecision(M)
    
    #Replace negative values for the model (impossible in the Poisson context)
    #with zero
    negativeMask              = (M < 0)
    if(len(negativeMask.nonzero()[0])>0):
      M[negativeMask]         = 0.0
    pass
    
    #Poisson loglikelihood statistic (Cash) is:
    # L = Sum ( M_i - D_i * log(M_i))   
    
    logM                      = self._evalLogM(M)
    
    #Evaluate v_i = D_i * log(M_i): if D_i = 0 then the product is zero
    #whatever value has log(M_i). Thus, initialize the whole vector v = {v_i}
    #to zero, then overwrite the elements corresponding to D_i > 0
    d_times_logM              = numpy.zeros(len(self.y))
    nonzeroMask               = (self.y > 0)
    d_times_logM[nonzeroMask] = self.y[nonzeroMask] * logM[nonzeroMask]
    
    logLikelihood             = numpy.sum( Mfixed - d_times_logM )

    return logLikelihood    
  pass
  
  def _fixPrecision(self,v):
    '''
      Round extremely small number inside v to the smallest usable
      number of the type corresponding to v. This is to avoid warnings
      and errors like underflows or overflows in math operations.
    '''
    tiny                      = numpy.float64(numpy.finfo(v[0]).tiny)
    zeroMask                  = (numpy.abs(v) <= tiny)
    if(len(zeroMask.nonzero()[0])>0):
      v[zeroMask]               = numpy.sign(v[zeroMask])*tiny
    
    return v, tiny
  pass
  
  def getFreeDerivs(self,parameters=None):
    '''
    Return the gradient of the logLikelihood for a given set of parameters (or the current
    defined one, if parameters=None)
    '''
    #The derivative of the logLikelihood statistic respect to parameter p is:
    # dC / dp = Sum [ (dM/dp)_i - D_i/M_i (dM/dp)_i]
    
    #Get the number of parameters and initialize the gradient to 0
    Nfree                     = self.model.getNumFreeParams()
    derivs                    = numpy.zeros(Nfree)
    
    #Set the parameters, if a new set has been provided
    if(parameters!=None):
      self.model.setParams(parameters)
    pass
    
    #Get the gradient of the model respect to the parameters
    modelDerivs               = self.model.getFreeDerivs(self.x)*self.exposure
    #Get the model
    M                         = self.model(self.x)*self.exposure
    
    M, tinyM                  = self._fixPrecision(M)
    
    #Compute y_divided_M = y/M: inizialize y_divided_M to zero
    #and then overwrite the elements for which y > 0. This is to avoid
    #possible underflow and overflow due to the finite precision of the
    #computer
    y_divided_M               = numpy.zeros(len(self.y))
    nonzero                   = (self.y > 0)
    y_divided_M[nonzero]      = self.y[nonzero]/M[nonzero]
       
    for p in range(Nfree):
      thisModelDerivs, tinyMd = self._fixPrecision(modelDerivs[p])
      derivs[p]               = numpy.sum(thisModelDerivs * (1.0 - y_divided_M) )
    pass
    
    return derivs
    
  pass
    
pass

class Polynomial(object):
  def __init__(self,params):
    self.params               = params
    self.degree               = len(params)-1
    
    #Build an empty covariance matrix
    self.covMatrix            = numpy.zeros([self.degree+1,self.degree+1])
  pass
  
  def horner(self, x):
    """A function that implements the Horner Scheme for evaluating a
    polynomial of coefficients *args in x."""
    result = 0
    for coefficient in self.params[::-1]:
        result = result * x + coefficient
    return result
  pass
  
  def __call__(self,x):
    return self.horner(x)
  pass
  
  def __str__(self):        
    #This is call by the print() command
    #Print results
    output                    = "\n------------------------------------------------------------"
    output                   += '\n| {0:^10} | {1:^20} | {2:^20} |'.format("COEFF","VALUE","ERROR")
    output                   += "\n|-----------------------------------------------------------"
    for i,parValue in enumerate(self.getParams()):
      output                 += '\n| {0:<10d} | {1:20.5g} | {2:20.5g} |'.format(i,parValue,math.sqrt(self.covMatrix[i,i]))
    pass
    output                   += "\n------------------------------------------------------------"
    
    return output
  pass
  
  def setParams(self,parameters):
    self.params               = parameters
  pass

  def getParams(self):
    return self.params
  pass
  
  def getNumFreeParams(self):
    return self.degree+1
  pass
  
  def getFreeDerivs(self,x):
    Npar                      = self.degree+1
    freeDerivs                = []
    for i in range(Npar):
      freeDerivs.append(map(lambda xx:pow(xx,i),x))
    pass
    return numpy.array(freeDerivs)
  pass
  
  def computeCovarianceMatrix(self,statisticGradient):
    self.covMatrix            = computeCovarianceMatrix(statisticGradient,self.params)
    #Check that the covariance matrix is positive-defined
    negativeElements          = (numpy.matrix.diagonal(self.covMatrix) < 0)
    if(len(negativeElements.nonzero()[0]) > 0):
      raise RuntimeError("Negative element in the diagonal of the covariance matrix. Try to reduce the polynomial grade.")
  pass  
  
  def getCovarianceMatrix(self):
    return self.covMatrix
  pass
  
  def integral(self,xmin,xmax):
    '''
    Evaluate the integral of the polynomial between xmin and xmax
    '''
    integralCoeff             = [0]
    integralCoeff.extend(map(lambda i:self.params[i-1]/float(i),range(1,self.degree+1+1)))
    
    integralPolynomial        = Polynomial(integralCoeff)
    
    return integralPolynomial(xmax) - integralPolynomial(xmin)
  pass
  
  def integralError(self,xmin,xmax):
    # Based on http://root.cern.ch/root/html/tutorials/fit/ErrorIntegral.C.html
    
    #Set the weights
    i_plus_1                  = numpy.array(range(1,self.degree+1+1),'d')
    def evalBasis(x):
      return (1/i_plus_1) * pow(x,i_plus_1)
    c                         = evalBasis(xmax) - evalBasis(xmin)
    
    #Compute the error on the integral
    err2                      = 0.0
    nPar                      = self.degree+1
    parCov                    = self.getCovarianceMatrix()
    for i in range(nPar):
      s                       = 0.0
      for j in range(nPar):
        s                    += parCov[i,j] * c[j]
      pass
      err2                   += c[i]*s
    pass
    
    return math.sqrt(err2)
  pass
  
pass
